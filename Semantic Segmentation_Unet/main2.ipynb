{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "UzpXeiVddIdi",
    "outputId": "64a2a2f0-dee6-41d7-9b6d-a2e4328acfde"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/CS543 CV/mp4\")\n",
    "os. getcwd() \n",
    "DATASET_PATH = 'data/sbd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oMHIGsCdckB"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/sbd/'\n",
    "\n",
    "class SegmentationDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Data loader for the Segmentation Dataset. If data loading is a bottleneck, \n",
    "    you may want to optimize this in for faster training. Possibilities include\n",
    "    pre-loading all images and annotations into memory before training, so as \n",
    "    to limit delays due to disk reads.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"train\", data_dir=DATASET_PATH):\n",
    "        assert(split in [\"train\", \"val\", \"test\"])\n",
    "        self.img_dir = os.path.join(data_dir, split)\n",
    "        self.classes = []\n",
    "        with open(os.path.join(data_dir, 'classes.txt'), 'r') as f:\n",
    "          for l in f:\n",
    "            self.classes.append(l.rstrip())\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.split = split\n",
    "        self.data = glob.glob(self.img_dir + '/*.jpg') \n",
    "        self.data = [os.path.splitext(l)[0] for l in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.data[index] + '.jpg')\n",
    "        gt = Image.open(self.data[index] + '.png')\n",
    "        \n",
    "        img = ToTensor()(img)\n",
    "        gt = torch.LongTensor(np.asarray(gt)).unsqueeze(0)\n",
    "        return img, gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet: Self Defined\n",
    "##### reference:Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "#TODO: design your own network here. The expectation is to write from scratch. But it's okay to get some inspiration \n",
    "#from conference paper. The bottom line is that you will not just copy code from other repo\n",
    "##########\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64 * factor, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6Dtn7yudjC8"
   },
   "outputs": [],
   "source": [
    "def segmentation_eval(gts, preds, classes, plot_file_name):\n",
    "    \"\"\"\n",
    "    @param    gts               numpy.ndarray   ground truth labels\n",
    "    @param    preds             numpy.ndarray   predicted labels\n",
    "    @param    classes           string          class names\n",
    "    @param    plot_file_name    string          plot file names\n",
    "    \"\"\"\n",
    "    ious, counts = compute_confusion_matrix(gts, preds)\n",
    "    aps = compute_ap(gts, preds)\n",
    "    plot_results(counts, ious, aps, classes, plot_file_name)\n",
    "    for i in range(len(classes)):\n",
    "        print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format(classes[i], aps[i], ious[i]))\n",
    "    print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format('mean', np.mean(aps), np.mean(ious)))\n",
    "    return aps, ious\n",
    "\n",
    "def plot_results(counts, ious, aps, classes, file_name):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    conf = counts / np.sum(counts, 1, keepdims=True)\n",
    "    conf = np.concatenate([conf, np.array(aps).reshape(-1,1), \n",
    "                           np.array(ious).reshape(-1,1)], 1)\n",
    "    conf = conf * 100.\n",
    "    sns.heatmap(conf, annot=True, ax=ax, fmt='3.0f') \n",
    "    arts = [] \n",
    "    # labels, title and ticks\n",
    "    _ = ax.set_xlabel('Predicted labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_ylabel('True labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_title('Confusion Matrix, mAP: {:5.1f}, mIoU: {:5.1f}'.format(\n",
    "      np.mean(aps)*100., np.mean(ious)*100.))\n",
    "    arts.append(_)\n",
    "    _ = ax.xaxis.set_ticklabels(classes + ['AP', 'IoU'], rotation=90)\n",
    "    arts.append(_)\n",
    "    _ = ax.yaxis.set_ticklabels(classes, rotation=0)\n",
    "    arts.append(_)\n",
    "    fig.savefig(file_name, bbox_inches='tight')\n",
    "\n",
    "def compute_ap(gts, preds):\n",
    "    aps = []\n",
    "    for i in range(preds.shape[1]):\n",
    "      ap, prec, rec = calc_pr(gts == i, preds[:,i:i+1,:,:])\n",
    "      aps.append(ap)\n",
    "    return aps\n",
    "\n",
    "def calc_pr(gt, out, wt=None):\n",
    "    gt = gt.astype(np.float64).reshape((-1,1))\n",
    "    out = out.astype(np.float64).reshape((-1,1))\n",
    "\n",
    "    tog = np.concatenate([gt, out], axis=1)*1.\n",
    "    ind = np.argsort(tog[:,1], axis=0)[::-1]\n",
    "    tog = tog[ind,:]\n",
    "    cumsumsortgt = np.cumsum(tog[:,0])\n",
    "    cumsumsortwt = np.cumsum(tog[:,0]-tog[:,0]+1)\n",
    "    prec = cumsumsortgt / cumsumsortwt\n",
    "    rec = cumsumsortgt / np.sum(tog[:,0])\n",
    "    ap = voc_ap(rec, prec)\n",
    "    return ap, rec, prec\n",
    "\n",
    "def voc_ap(rec, prec):\n",
    "    rec = rec.reshape((-1,1))\n",
    "    prec = prec.reshape((-1,1))\n",
    "    z = np.zeros((1,1)) \n",
    "    o = np.ones((1,1))\n",
    "    mrec = np.vstack((z, rec, o))\n",
    "    mpre = np.vstack((z, prec, z))\n",
    "\n",
    "    mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
    "    I = np.where(mrec[1:] != mrec[0:-1])[0]+1;\n",
    "    ap = np.sum((mrec[I] - mrec[I-1])*mpre[I])\n",
    "    return ap\n",
    "\n",
    "def compute_confusion_matrix(gts, preds):\n",
    "    preds_cls = np.argmax(preds, 1)\n",
    "    gts = gts[:,0,:,:]\n",
    "    conf = confusion_matrix(gts.ravel(), preds_cls.ravel())\n",
    "    inter = np.diag(conf)\n",
    "    union = np.sum(conf, 0) + np.sum(conf, 1) - np.diag(conf)\n",
    "    union = np.maximum(union, 1)\n",
    "    return inter / union, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUEu8F7Je27N"
   },
   "outputs": [],
   "source": [
    "# Colab has GPUs, you will have to move tensors and models to GPU.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "TOTAL_CLASSES=9\n",
    "model = MyModel(3,TOTAL_CLASSES).to(device) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-8, momentum=0.9)\n",
    "\n",
    "train_loss_over_epochs = []\n",
    "val_accuracy_over_epochs = []\n",
    "\n",
    "EPOCHS = 50\n",
    "train_dataset = SegmentationDataset(split='train')\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=1,  shuffle=True, num_workers=0, drop_last=True)\n",
    "IS_GPU = True\n",
    "\n",
    "val_dataset = SegmentationDataset(split='val')\n",
    "valloader = data.DataLoader(val_dataset, batch_size=1,  shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "499ec3d07e2c4cda946747cef76fccc5",
      "db1a3cafd15743979f8edcdd3d375bbe",
      "27320da419a14b699a0a51edef10e0cc",
      "5f7eb70e517a4fc485971590389d2f17",
      "abf97577c9a842fca48bb21a867f8b6c",
      "1b9cfcde82d84781b5f9a2676780bb0c",
      "c3d54bd79c6f4f7b9bf9f0d2e59dfcfd",
      "bd1bc8286f584d1090676e3dcd3b5b03"
     ]
    },
    "colab_type": "code",
    "id": "kewcGiHgs3sP",
    "outputId": "de1cae3b-6d97-4faf-aed2-60236a8858c0"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(dataloader, is_gpu,model):\n",
    "    \"\"\" Util function to calculate val set accuracy,\n",
    "    both overall and per class accuracy\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): val set \n",
    "        is_gpu (bool): whether to run on GPU\n",
    "    Returns:\n",
    "        tuple: (overall accuracy, class level accuracy)\n",
    "    \"\"\"    \n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        if is_gpu:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = model(Variable(images))\n",
    "        a, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "\n",
    "        right=torch.sum(c)\n",
    "        total += (labels.size(2)*labels.size(3))\n",
    "        correct += right\n",
    "\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okJ-P_JGesKq"
   },
   "outputs": [],
   "source": [
    "# This is a trivial semantic segmentor. For eqch pixel location it computes the \n",
    "# distribution of the class label in the training set and uses that as the \n",
    "# prediction. Quite unsuprisingly it doesn't perform very well. Though we provide\n",
    "# this code so that you can understand the data formats for the benchmarking \n",
    "# functions.\n",
    "def simple_train():\n",
    "    train_dataset = SegmentationDataset(split='train')\n",
    "    train_dataloader = data.DataLoader(train_dataset, batch_size=1, \n",
    "                                       shuffle=True, num_workers=0, \n",
    "                                       drop_last=True)\n",
    "    counts = np.zeros((train_dataset.n_classes, 224, 288))\n",
    "    N = 0\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "      img, gt = batch\n",
    "      gt = gt.cpu().numpy()\n",
    "      for j in range(train_dataset.n_classes):\n",
    "          counts[j,:,:] += gt[0,0,:,:] == j\n",
    "      N += 1\n",
    "    model = counts / N\n",
    "    return model\n",
    "\n",
    "def simple_predict(split, model):\n",
    "    dataset = SegmentationDataset(split=split, data_dir=DATASET_PATH)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                                 num_workers=0, drop_last=False)\n",
    "    gts, preds = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "      img, gt = batch\n",
    "      gt = gt.cpu().numpy()\n",
    "      gts.append(gt[0,:,:,:])\n",
    "      preds.append(model)\n",
    "                              \n",
    "    gts = np.array(gts)\n",
    "    preds = np.array(preds)\n",
    "    return gts, preds, list(dataset.classes)\n",
    "\n",
    "\n",
    "def predict(split, model):\n",
    "    dataset = SegmentationDataset(split=split, data_dir=DATASET_PATH)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                                 num_workers=0, drop_last=False)\n",
    "    gts, preds = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "      img, gt = batch\n",
    "       \n",
    "      gt = gt.cpu().numpy()\n",
    "    \n",
    "      gts.append(gt[0,:,:,:])\n",
    "      outputs= model(img.to(device))\n",
    "#       print(outputs.shape)\n",
    "      sf=nn.Softmax(dim=1)\n",
    "      outputs= sf(outputs)\n",
    "\n",
    "    \n",
    "      outputs=outputs.detach().cpu().numpy()[0,:,:,:]\n",
    "#       print(outputs.shape)\n",
    "#       pl=np.argmax(outputs, axis=1)\n",
    "    \n",
    "      preds.append(outputs)\n",
    "                              \n",
    "    gts = np.array(gts)\n",
    "    preds = np.array(preds)\n",
    "    return gts, preds, list(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3sJiJPb3huXK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n",
      "torch.Size([1, 1, 224, 288])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-300bf630717a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#         break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Normalizing the loss by the total number of train batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# TODO: Implement your training cycles, make sure you evaluate on validation \n",
    "# dataset and compute evaluation metrics every so often. \n",
    "# You may also want to save models that perform well.\n",
    "for epoch in tqdm(range(EPOCHS), total=EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, Data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = Data\n",
    "        print(labels.shape)\n",
    "        if IS_GPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        outputs=outputs.permute(0,2,3,1)\n",
    "        \n",
    "        loss = criterion(outputs.view(-1,9), labels.view(-1))\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         break\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    running_loss/=len(trainloader)\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss))\n",
    "\n",
    "    # Scale of 0.0 to 100.0\n",
    "    # Calculate validation set accuracy of the existing model\n",
    "    val_accuracy = \\\n",
    "        calculate_accuracy(valloader, IS_GPU,model)\n",
    "    print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
    "\n",
    "    # Optionally print classwise accuracies\n",
    "#     for c_i in range(TOTAL_CLASSES):\n",
    "#         print('Accuracy of %5s : %2d %%' % (\n",
    "#             classes[c_i], 100 * val_classwise_accuracy[c_i]))\n",
    "\n",
    "    train_loss_over_epochs.append(running_loss)\n",
    "#     val_accuracy_over_epochs.append(val_accuracy)\n",
    "# -----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HA9WZ-Nqh1Ka"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: Evaluate your result, and report Mean average precision on test dataset \n",
    "# using provided helper function. Here we show how we can train and evaluate the \n",
    "# simple model that we provided on the validation set. You will want to report\n",
    "# performance on the validation set for the variants you tried, and the \n",
    "# performance of the final model on the test set.\n",
    "# model = simple_train()\n",
    "# gts, preds, classes = simple_predict('val', model)\n",
    "gts, preds, classes = predict('test', model)\n",
    "aps, ious = segmentation_eval(gts, preds, classes, 'cs543-simple-unet.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet based on Pre-Trained Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_Res18(nn.Module):\n",
    "    def __init__(self, inC,MiddleC,outC):\n",
    "        super(decoder_Res18,self).__init__()\n",
    "        \n",
    "        self.sequence=nn.Sequential(\n",
    "            nn.Conv2d(inC, MiddleC,3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(MiddleC),\n",
    "            nn.ConvTranspose2d(MiddleC,outC,kernel_size=4,stride=2,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(outC)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.sequence(x)\n",
    "    \n",
    "\n",
    "\n",
    "class Unet_Res18(nn.Module):\n",
    "    def __init__(self,number_class=9, pretrained=True, Dropout=0.3):\n",
    "    \n",
    "        super(Unet_Res18,self).__init__()\n",
    "        if pretrained:\n",
    "            self.encoder= models.resnet18(pretrained=pretrained)\n",
    "        else:\n",
    "            print('please defined your own model here')\n",
    "\n",
    "        # fix the parameters in resnet18\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.number_class=number_class\n",
    "        self.Dropout=Dropout\n",
    "        self.relu= nn.ReLU(inplace= True)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.dropout_2d = nn.Dropout2d(p=Dropout)\n",
    "        self.conv1=nn.Sequential(self.encoder.conv1,\n",
    "                                 self.encoder.bn1,\n",
    "                                 self.encoder.relu,\n",
    "                                 self.pool\n",
    "                                )\n",
    "        self.conv2=self.encoder.layer1\n",
    "        self.conv3=self.encoder.layer2\n",
    "        self.conv4=self.encoder.layer3\n",
    "        self.conv5=self.encoder.layer4\n",
    "        \n",
    "        self.decov=decoder_Res18(512, 512, 256)\n",
    "        self.decov5=decoder_Res18(512+256, 512, 256)\n",
    "        self.decov4=decoder_Res18(256+256, 512, 256)\n",
    "        self.decov3=decoder_Res18(128+256, 256, 64)\n",
    "        self.decov2=decoder_Res18(64+64, 128, 128)\n",
    "        self.decov1=decoder_Res18(128, 128, 32)\n",
    "        self.decov0=nn.Sequential(\n",
    "            nn.Conv2d(32,self.number_class,kernel_size=1)\n",
    "        )\n",
    "            \n",
    "        \n",
    "    def Pad_Same(self, x1, x2):\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)  \n",
    "        return x\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.dropout_2d(self.conv2(conv1))\n",
    "        conv3 = self.dropout_2d(self.conv3(conv2))\n",
    "        conv4 = self.dropout_2d(self.conv4(conv3))\n",
    "        conv5 = self.dropout_2d(self.conv5(conv4))\n",
    "\n",
    "        mid = self.decov(self.pool(conv5))\n",
    "\n",
    "        decov5= self.decov5( self.Pad_Same(mid,conv5))\n",
    "        decov4= self.decov4( self.Pad_Same(decov5, conv4))\n",
    "        decov3= self.decov3( self.Pad_Same(decov4, conv3))\n",
    "        decov2= self.decov2( self.Pad_Same(decov3, conv2))\n",
    "        decov1= self.decov1( self.dropout_2d(decov2))\n",
    "        \n",
    "        output= self.decov0( decov1)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab has GPUs, you will have to move tensors and models to GPU.\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "TOTAL_CLASSES=9\n",
    "model2 = Unet_Res18(TOTAL_CLASSES).to(device) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model2.parameters(), lr=0.001, weight_decay=1e-8, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model2 .parameters(), lr=0.001, weight_decay=5e-4)\n",
    "train_loss_over_epochs = []\n",
    "val_accuracy_over_epochs = []\n",
    "\n",
    "EPOCHS = 35\n",
    "train_dataset = SegmentationDataset(split='train')\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=1,  shuffle=True, num_workers=0, drop_last=True)\n",
    "IS_GPU = True\n",
    "\n",
    "val_dataset = SegmentationDataset(split='val')\n",
    "valloader = data.DataLoader(val_dataset, batch_size=1,  shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), total=EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, Data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = Data\n",
    "        if IS_GPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model2(inputs)\n",
    "\n",
    "\n",
    "        outputs=outputs.permute(0,2,3,1)\n",
    "        \n",
    "        loss = criterion(outputs.view(-1,9), labels.view(-1))\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    running_loss/=len(trainloader)\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss))\n",
    "\n",
    "    # Calculate validation set accuracy of the existing model\n",
    "    val_accuracy = \\\n",
    "        calculate_accuracy(valloader, IS_GPU, model2)\n",
    "    print('Accuracy of the network on the val images: %d %%' % (val_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "    train_loss_over_epochs.append(running_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts, preds, classes = predict('test', model2)\n",
    "aps, ious = segmentation_eval(gts, preds, classes, 'cs543-Resnet-unet-33.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“CS543_MP4_part2_starter_code.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1b9cfcde82d84781b5f9a2676780bb0c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27320da419a14b699a0a51edef10e0cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9cfcde82d84781b5f9a2676780bb0c",
      "max": 572,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abf97577c9a842fca48bb21a867f8b6c",
      "value": 0
     }
    },
    "499ec3d07e2c4cda946747cef76fccc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_27320da419a14b699a0a51edef10e0cc",
       "IPY_MODEL_5f7eb70e517a4fc485971590389d2f17"
      ],
      "layout": "IPY_MODEL_db1a3cafd15743979f8edcdd3d375bbe"
     }
    },
    "5f7eb70e517a4fc485971590389d2f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd1bc8286f584d1090676e3dcd3b5b03",
      "placeholder": "​",
      "style": "IPY_MODEL_c3d54bd79c6f4f7b9bf9f0d2e59dfcfd",
      "value": " 0/572 [00:00&lt;?, ?it/s]"
     }
    },
    "abf97577c9a842fca48bb21a867f8b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bd1bc8286f584d1090676e3dcd3b5b03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3d54bd79c6f4f7b9bf9f0d2e59dfcfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db1a3cafd15743979f8edcdd3d375bbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
